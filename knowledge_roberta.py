# -*- coding: utf-8 -*-
"""Knowgraph_finalxxxx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bcT-O5YeOo7DcstsgO9WsyA7HUPP3Rwp

# KG
"""


from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
import numpy as np
# -*- encoding:utf-8 -*-

# Special token ids.
PAD_ID = 1
UNK_ID = 3
CLS_ID = 0
SEP_ID = 2
MASK_ID = 50264

# Special token words.
PAD_TOKEN = '<pad>'
UNK_TOKEN = '<unk>'
CLS_TOKEN = '<s>'
SEP_TOKEN = '</s>'
MASK_TOKEN = '<mask>'

#Never split text
NEVER_SPLIT_TAG = [PAD_TOKEN, UNK_TOKEN, CLS_TOKEN, SEP_TOKEN, MASK_TOKEN]

import spacy
nlp = spacy.load("en_core_web_sm")
def entities_search2(sent, lktb, nlp=nlp, max_entities = 4):
    skip = 0
    all_tokens = []
    doc = nlp(sent)
    lst_chunks = [chunk.text for chunk in doc.noun_chunks]

    lst_tokens = []
    for tkn in doc:
        if tkn.pos_!='PUNCT':
            lst_tokens.append(tkn.text)
    # print('list token:', lst_tokens)
    # print('list chunks:', lst_chunks)

    for (idx, token) in enumerate(lst_tokens):
        if skip==0:
            token = token.strip()
            check = []
            for i in range(len(lst_chunks)):
                if token==lst_chunks[i].split(' ')[0]:
                    check.append(i)
            if check==[]:
                en = list(lktb.get(token, []))[:max_entities]
                all_tokens.append([token, en])
                skip = 0
            else:
                len_max = -999
                log = 1
                for i in check:
                    count = 1
                    if len(lst_chunks[i].split(' '))==1:
                        count = 1
                    else:
                        for j in range(1, len(lst_chunks[i].split(' '))):
                            if idx+j<len(lst_tokens):
                                if (lst_chunks[i].split(' ')[j]==lst_tokens[idx+j]):
                                    count = count + 1
                            else:
                                count = 0
                                break
                    if count>len_max:
                        len_max = count
                        log = i
                        skip = len_max - 1
                token = lst_chunks[log]
                en = list(lktb.get(token, []))[:max_entities]
                all_tokens.append([token, en])
        else:
            skip = skip - 1
        # print('all:', all_tokens)
    return all_tokens

def make_vm_sent(sent, lktb,nlp = nlp, max_length = 256):
    pos = [0]
    token_id = []
    token_all = []
    doc = nlp(sent)
    list_tokens = entities_search2(sent,lktb, nlp = nlp)
    if list_tokens==[]:
        list_tokens = [[token.text, []] for token in doc]
    for item in list_tokens:
        if item[1]==[]:
            tokens = tokenizer.tokenize(item[0].strip())
            token_all = token_all+ tokens
            token_id = token_id +[0]*len(tokens)
        else:
            if len(item[1])==1:
                tokens = tokenizer.tokenize(item[0].strip())
                token_all = token_all+ tokens
                token_id = token_id +[1]*len(tokens)
                en_tokens = tokenizer.tokenize(item[1][0].strip())
                token_all = token_all+ en_tokens
                token_id = token_id +[2]*len(en_tokens)
            else:
                tokens = tokenizer.tokenize(item[0].strip())
                token_all = token_all+ tokens
                token_id = token_id +[1]*len(tokens)
                len_entities = len(item[1])
                en_tokens = []
                len_id = 0
                for i in range(len_entities):
                    en_tokens = en_tokens + tokenizer.tokenize(item[1][i].strip())
                    len_id = len_id + len(tokenizer.tokenize(item[1][i].strip()))
                token_all = token_all+ en_tokens
                token_id = token_id +[2]*len_id
    # print('token_id:', token_id)
    #add [CLS]
    total_token = len(token_id)+1
    token_id.insert(0, -1)
    token_all.insert(0, '<s>')
        
    #cal VM
    visible_matrix = np.zeros((total_token, total_token))
    for i in range(total_token):
        check = False
        for j in range(total_token):
            if token_id[i]==-1:
                if token_id[j]==0 or token_id[j]==1 or token_id[j]==-1:
                    visible_matrix[i][j] = 1
            elif token_id[i]==0:
                if token_id[j]==0 or token_id[j]==1 or token_id[j]==-1:
                    visible_matrix[i][j] = 1
            elif token_id[i]==1:
                if token_id[j]==0 or token_id[j]==1 or token_id[j]==-1:
                    visible_matrix[i][j] = 1
                else:
                    if check==False and j>i:
                        visible_matrix[i][j] = 1
                        if j+1<total_token:
                            if token_id[j+1]!=2:
                                check = True
                                
            else:
                if i==j:
                    visible_matrix[i][j] = 1
                    check2 = False
                    for k in range(1,j-1):
                        if token_id[j-k]==1 and check2==False:
                            visible_matrix[i][j-k] = 1
                            if token_id[j-k-1]!=1:
                                check2=True
                        if check2==True:
                            break
                    for k in range(1,j-1):
                        if token_id[j-k]==2:
                            visible_matrix[i][j-k] = 1
                        else:
                            break
                            
                elif j>i:
                    if token_id[j]==2:
                        visible_matrix[i][j] = 1
                    else:
                        break

    #cal pos
    main_id = 1
    tree_id = 0
    check = False
    for i in range(1, len(token_id)):
        if token_id[i]==0 or token_id[i]==1:
            pos.append(main_id)
            main_id = main_id+1
        else:
            if check==False:
                for j in range(1,i):
                    if token_id[i-j]==1 and (i-j>=1):
                        tree_id = pos[i-j]+1
                        pos.append(tree_id)
                        check = True
                        tree_id = tree_id+1
                        break
            else:
                pos.append(tree_id)
                tree_id = tree_id + 1
                if (i+1)<len(token_id) and token_id[i+1]!=2:
                    check = False
    pos = pos[1:]
        

    #add pad
    # when using roberta_tokenizer("Hello, I am Khang", padding = "max_length", truncation=  True, max_length = 20)
    # >> input_ids : [0, 31414, 6, 1308, 766, 16, 2218, 1097, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    # >> attention: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    # Although <pad_id> = 1 but semantic word is still set to 1 in attention_mask and 0 is otherwise. 
    # Therefore, visible matrix need to be like that, semantic words is presented by 1, unrelated words or non-semantic is 0
    # pos is position of each word in input sentence (sentence tree), therefore each element needs to be in order, not affected by pad_id
    # token has PAD_TOKEN = <pad> (pad_token of roberta), it is correct. 
    if len(token_all) <= max_length:
        pad_num = max_length - total_token
        token_all += [PAD_TOKEN] * pad_num
        pos += [max_length - 1] * (pad_num+1)
        visible_matrix = np.pad(visible_matrix, ((0, pad_num), (0, pad_num)), 'constant')  # pad 0
    else:
        token_all = token_all[:max_length]
        pos = pos[:max_length]
        visible_matrix = visible_matrix[:max_length, :max_length]

    return token_all, pos, visible_matrix


def find_sublist(lst, slst):
    a = len(lst)
    b = len(slst)
    for i in range(0, a):
        check = False
        for j in range(0, b):
            if lst[i+j] == slst[j]:
                check = True
            else:
                check= False
                break
        if check==True:
            return i
    return -1

# Build Knowledge Graph

class KnowledgeGraph(object):
    def __init__(self, txt_file, predicate=True):
        self.predicate = predicate
        self.txt_file_path = txt_file
        self.lookup_table = self._create_lookup_table()
        self.special_tags = set(NEVER_SPLIT_TAG)

    
    def _create_lookup_table(self):
        lookup_table = {}
        print("[KnowledgeGraph] Loading spo from {}".format(self.txt_file_path))
        with open(self.txt_file_path, encoding="utf8", errors='ignore') as f:
            for line in f:
                try:
                    subj, pred, obje = line.strip().split("\t")
                except:
                    #print("[KnowledgeGraph] Bad spo:", line)
                    continue
                subj, pred, obje = subj.strip(), pred.strip(), obje.strip()
                subj, pred, obje = subj.lower(), pred.lower(), obje.lower()
                if self.predicate:
                    value = pred +' ' + obje
                else:
                    value = obje
                if subj in lookup_table.keys():
                    lookup_table[subj].add(value)
                else:
                    lookup_table[subj] = set([value])
        return lookup_table

    def add_knowledge_with_vm(self, sent_batch, add_pad=True, max_length=256):
        """
        input: sent_batch - list of sentences, e.g., ["abcd", "efgh"]
        return: know_sent_batch - list of sentences with entites embedding
                position_batch - list of position index of each character.
                visible_matrix_batch - list of visible matrixs
                seg_batch - list of segment tags
        """
        know_sent_batch = []
        position_batch = []
        visible_matrix_batch = []
        for sent in sent_batch:
            token_all, pos, visible_matrix = make_vm_sent(sent, lktb=self.lookup_table, max_length=max_length)
            know_sent_batch.append(token_all)
            position_batch.append(pos)
            visible_matrix_batch.append(visible_matrix)
        return know_sent_batch, position_batch, visible_matrix_batch

"""# Test"""

"""eye redness, pediatric rabies, congestion of the throat"""

